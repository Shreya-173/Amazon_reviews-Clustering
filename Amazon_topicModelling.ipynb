{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "soy8Hbc6ItNK"
      },
      "source": [
        "                 Name: Shreya            Andrew ID: sshreya             Zillow Zestimate Prediction     "
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "PibKcv4OIzOp"
      },
      "source": [
        "### The Amazon online marketplace relies on product reviews written by customers based on experience.The Amazon review dataset has over 21.9 million product reviews for 1.2 million products. We will hypothesize the classes of concerns expressed by customers through unsupervised learning using Latent Dirichlet Allocation (LDA) to generate topic models from the text. Each cluster is described by words that predict membership in the cluster. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HMZpCGFC7aFc",
        "outputId": "cd723105-d1bd-4236-8dcc-88d4e95f60ac"
      },
      "outputs": [],
      "source": [
        "import csv\n",
        "import pandas as pd\n",
        "import json\n",
        "import random\n",
        "\n",
        "# from google.colab import drive\n",
        "\n",
        "# drive.mount('/content/gdrive/', force_remount=True)\n",
        "\n",
        "# # !pip install numpy\n",
        "# %cd /content/gdrive/My Drive/Amazon"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "xYefS7CZ9CLb"
      },
      "outputs": [],
      "source": [
        "# set the file path and load the json file to like of dfs in chunks of 10000\n",
        "file_path = \"Home_and_Kitchen.json\"\n",
        "dfs = []\n",
        "for chunk in pd.read_json(file_path, lines=True, chunksize=10000):\n",
        "    dfs.append(chunk.sample(frac=0.1))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "EPX9rpw87iFa"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "2192857"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Concatenate the list of dataframes into a single dataframe\n",
        "review_df = pd.concat(dfs, ignore_index=True)\n",
        "len(review_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 565
        },
        "id": "RxTGHEWg8BMg",
        "outputId": "7ce83967-b2ed-4d3a-f108-fef00c74fcdd"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>overall</th>\n",
              "      <th>vote</th>\n",
              "      <th>verified</th>\n",
              "      <th>reviewTime</th>\n",
              "      <th>reviewerID</th>\n",
              "      <th>asin</th>\n",
              "      <th>reviewerName</th>\n",
              "      <th>reviewText</th>\n",
              "      <th>summary</th>\n",
              "      <th>unixReviewTime</th>\n",
              "      <th>style</th>\n",
              "      <th>image</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>5</td>\n",
              "      <td>NaN</td>\n",
              "      <td>True</td>\n",
              "      <td>03 13, 2017</td>\n",
              "      <td>A1G91EWH4WCOHS</td>\n",
              "      <td>1933682612</td>\n",
              "      <td>JESSICA HAYES</td>\n",
              "      <td>Bought this for my stepdaughter when she start...</td>\n",
              "      <td>A classic</td>\n",
              "      <td>1489363200</td>\n",
              "      <td>{'Format:': ' Toy'}</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>4</td>\n",
              "      <td>NaN</td>\n",
              "      <td>False</td>\n",
              "      <td>09 14, 2017</td>\n",
              "      <td>A35JNJZKZY0S8Q</td>\n",
              "      <td>B00002N601</td>\n",
              "      <td>Houstonian</td>\n",
              "      <td>Generally speaking, this product is good enoug...</td>\n",
              "      <td>Only one little thing not that good</td>\n",
              "      <td>1505347200</td>\n",
              "      <td>{'Size:': ' 6 qt'}</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>5</td>\n",
              "      <td>NaN</td>\n",
              "      <td>True</td>\n",
              "      <td>11 10, 2016</td>\n",
              "      <td>A4P9EJZ09I1BP</td>\n",
              "      <td>710105482X</td>\n",
              "      <td>Carlpak1</td>\n",
              "      <td>There are 100's of uses for this product howev...</td>\n",
              "      <td>Useful</td>\n",
              "      <td>1478736000</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4</td>\n",
              "      <td>NaN</td>\n",
              "      <td>True</td>\n",
              "      <td>01 30, 2013</td>\n",
              "      <td>A8THV1OK4VNFP</td>\n",
              "      <td>0983124248</td>\n",
              "      <td>K. Christensen</td>\n",
              "      <td>After years of just putting stickers on paper ...</td>\n",
              "      <td>Sticker book</td>\n",
              "      <td>1359504000</td>\n",
              "      <td>{'Format:': ' Spiral-bound'}</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5</td>\n",
              "      <td>NaN</td>\n",
              "      <td>True</td>\n",
              "      <td>12 17, 2013</td>\n",
              "      <td>A2V1K4KARXFH42</td>\n",
              "      <td>B0000224M6</td>\n",
              "      <td>kd</td>\n",
              "      <td>Smells good. I'm happy,\\nSunflowers are a nice...</td>\n",
              "      <td>Nice</td>\n",
              "      <td>1387238400</td>\n",
              "      <td>{'Size:': ' 428'}</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   overall vote  verified   reviewTime      reviewerID        asin  \\\n",
              "0        5  NaN      True  03 13, 2017  A1G91EWH4WCOHS  1933682612   \n",
              "1        4  NaN     False  09 14, 2017  A35JNJZKZY0S8Q  B00002N601   \n",
              "2        5  NaN      True  11 10, 2016   A4P9EJZ09I1BP  710105482X   \n",
              "3        4  NaN      True  01 30, 2013   A8THV1OK4VNFP  0983124248   \n",
              "4        5  NaN      True  12 17, 2013  A2V1K4KARXFH42  B0000224M6   \n",
              "\n",
              "     reviewerName                                         reviewText  \\\n",
              "0   JESSICA HAYES  Bought this for my stepdaughter when she start...   \n",
              "1      Houstonian  Generally speaking, this product is good enoug...   \n",
              "2        Carlpak1  There are 100's of uses for this product howev...   \n",
              "3  K. Christensen  After years of just putting stickers on paper ...   \n",
              "4              kd  Smells good. I'm happy,\\nSunflowers are a nice...   \n",
              "\n",
              "                               summary  unixReviewTime  \\\n",
              "0                            A classic      1489363200   \n",
              "1  Only one little thing not that good      1505347200   \n",
              "2                               Useful      1478736000   \n",
              "3                         Sticker book      1359504000   \n",
              "4                                 Nice      1387238400   \n",
              "\n",
              "                          style image  \n",
              "0           {'Format:': ' Toy'}   NaN  \n",
              "1            {'Size:': ' 6 qt'}   NaN  \n",
              "2                           NaN   NaN  \n",
              "3  {'Format:': ' Spiral-bound'}   NaN  \n",
              "4             {'Size:': ' 428'}   NaN  "
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "review_df.head()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Since we are focusing on the classes of concerns expressed by customers\n",
        "#### Check for all reviews and their count and remove the ones with overall rating 4 and above"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UdcSzAABVJAh",
        "outputId": "bc8117ad-7d36-43d0-b9c0-4cb118f59374"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "5    1415539\n",
            "4     303188\n",
            "1     201343\n",
            "3     161625\n",
            "2     111162\n",
            "Name: overall, dtype: int64\n"
          ]
        }
      ],
      "source": [
        "review_df['overall'].unique()\n",
        "rating_counts = review_df['overall'].value_counts()\n",
        "\n",
        "# print the result\n",
        "print(rating_counts)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H6mm-rO5WNaU",
        "outputId": "f486bd11-0651-4ac7-cefa-04dc5427f60d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "474130\n"
          ]
        }
      ],
      "source": [
        "indexAge = review_df[ (review_df['overall'] >= 4) ].index\n",
        "review_df.drop(indexAge , inplace=True)\n",
        "review_df\n",
        "print(len(review_df))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VGhUr8Uy_RVS",
        "outputId": "d00ac907-f540-45c5-ee16-e0235096bb8d"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "overall                0\n",
              "vote              373831\n",
              "verified               0\n",
              "reviewTime             0\n",
              "reviewerID             0\n",
              "asin                   0\n",
              "reviewerName          35\n",
              "reviewText           186\n",
              "summary               69\n",
              "unixReviewTime         0\n",
              "style             221735\n",
              "image             455253\n",
              "dtype: int64"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "review_df.isnull().sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "PjJm2XLrkebX"
      },
      "outputs": [],
      "source": [
        "## Remove the rows with missing reviewText\n",
        "\n",
        "review_df = review_df.dropna(subset = ['reviewText'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9834pbvPlQyl",
        "outputId": "a62d38ee-a93a-4b57-b7c2-7a6238f3aadf"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "473944"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(review_df)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "83mpXiXV9Opl"
      },
      "source": [
        "### Stemming of each tokenized list of each review post removing all the stop words"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6tg-_mP4ogH6",
        "outputId": "beb46018-7504-422b-f1f8-f328193d1138"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pattern in c:\\users\\kashy\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (3.6)\n",
            "Requirement already satisfied: mysqlclient in c:\\users\\kashy\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pattern) (2.1.1)\n",
            "Requirement already satisfied: beautifulsoup4 in c:\\users\\kashy\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pattern) (4.11.1)\n",
            "Requirement already satisfied: pdfminer.six in c:\\users\\kashy\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pattern) (20221105)\n",
            "Requirement already satisfied: nltk in c:\\users\\kashy\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pattern) (3.8.1)\n",
            "Requirement already satisfied: scipy in c:\\users\\kashy\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pattern) (1.9.3)\n",
            "Requirement already satisfied: cherrypy in c:\\users\\kashy\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pattern) (18.8.0)\n",
            "Requirement already satisfied: requests in c:\\users\\kashy\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pattern) (2.25.1)\n",
            "Requirement already satisfied: backports.csv in c:\\users\\kashy\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pattern) (1.0.7)\n",
            "Requirement already satisfied: lxml in c:\\users\\kashy\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pattern) (4.9.1)\n",
            "Requirement already satisfied: python-docx in c:\\users\\kashy\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pattern) (0.8.11)\n",
            "Requirement already satisfied: numpy in c:\\users\\kashy\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pattern) (1.24.2)\n",
            "Requirement already satisfied: feedparser in c:\\users\\kashy\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pattern) (6.0.10)\n",
            "Requirement already satisfied: future in c:\\users\\kashy\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pattern) (0.18.3)\n",
            "Requirement already satisfied: soupsieve>1.2 in c:\\users\\kashy\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from beautifulsoup4->pattern) (2.3.1)\n",
            "Requirement already satisfied: more-itertools in c:\\users\\kashy\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from cherrypy->pattern) (9.0.0)\n",
            "Requirement already satisfied: zc.lockfile in c:\\users\\kashy\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from cherrypy->pattern) (2.0)\n",
            "Requirement already satisfied: jaraco.collections in c:\\users\\kashy\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from cherrypy->pattern) (3.8.0)\n",
            "Requirement already satisfied: portend>=2.1.1 in c:\\users\\kashy\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from cherrypy->pattern) (3.1.0)\n",
            "Requirement already satisfied: cheroot>=8.2.1 in c:\\users\\kashy\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from cherrypy->pattern) (9.0.0)\n",
            "Requirement already satisfied: sgmllib3k in c:\\users\\kashy\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from feedparser->pattern) (1.0.0)\n",
            "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\kashy\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from nltk->pattern) (2022.10.31)\n",
            "Requirement already satisfied: joblib in c:\\users\\kashy\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from nltk->pattern) (1.2.0)\n",
            "Requirement already satisfied: tqdm in c:\\users\\kashy\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from nltk->pattern) (4.64.1)\n",
            "Requirement already satisfied: click in c:\\users\\kashy\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from nltk->pattern) (8.1.3)\n",
            "Requirement already satisfied: cryptography>=36.0.0 in c:\\users\\kashy\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pdfminer.six->pattern) (38.0.4)\n",
            "Requirement already satisfied: charset-normalizer>=2.0.0 in c:\\users\\kashy\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pdfminer.six->pattern) (2.0.12)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in c:\\users\\kashy\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests->pattern) (4.0.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\kashy\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests->pattern) (1.26.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\kashy\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests->pattern) (2021.10.8)\n",
            "Requirement already satisfied: idna<3,>=2.5 in c:\\users\\kashy\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests->pattern) (2.10)\n",
            "Requirement already satisfied: jaraco.functools in c:\\users\\kashy\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from cheroot>=8.2.1->cherrypy->pattern) (3.5.2)\n",
            "Requirement already satisfied: six>=1.11.0 in c:\\users\\kashy\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from cheroot>=8.2.1->cherrypy->pattern) (1.16.0)\n",
            "Requirement already satisfied: cffi>=1.12 in c:\\users\\kashy\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from cryptography>=36.0.0->pdfminer.six->pattern) (1.15.0)\n",
            "Requirement already satisfied: tempora>=1.8 in c:\\users\\kashy\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from portend>=2.1.1->cherrypy->pattern) (5.2.1)\n",
            "Requirement already satisfied: colorama in c:\\users\\kashy\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from click->nltk->pattern) (0.4.6)\n",
            "Requirement already satisfied: jaraco.text in c:\\users\\kashy\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from jaraco.collections->cherrypy->pattern) (3.11.1)\n",
            "Requirement already satisfied: jaraco.classes in c:\\users\\kashy\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from jaraco.collections->cherrypy->pattern) (3.2.3)\n",
            "Requirement already satisfied: setuptools in c:\\users\\kashy\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from zc.lockfile->cherrypy->pattern) (67.4.0)\n",
            "Requirement already satisfied: pycparser in c:\\users\\kashy\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from cffi>=1.12->cryptography>=36.0.0->pdfminer.six->pattern) (2.21)\n",
            "Requirement already satisfied: pytz in c:\\users\\kashy\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tempora>=1.8->portend>=2.1.1->cherrypy->pattern) (2022.1)\n",
            "Requirement already satisfied: autocommand in c:\\users\\kashy\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from jaraco.text->jaraco.collections->cherrypy->pattern) (2.2.2)\n",
            "Requirement already satisfied: jaraco.context>=4.1 in c:\\users\\kashy\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from jaraco.text->jaraco.collections->cherrypy->pattern) (4.3.0)\n",
            "Requirement already satisfied: inflect in c:\\users\\kashy\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from jaraco.text->jaraco.collections->cherrypy->pattern) (6.0.2)\n",
            "Requirement already satisfied: pydantic>=1.9.1 in c:\\users\\kashy\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from inflect->jaraco.text->jaraco.collections->cherrypy->pattern) (1.10.5)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in c:\\users\\kashy\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pydantic>=1.9.1->inflect->jaraco.text->jaraco.collections->cherrypy->pattern) (4.4.0)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to\n",
            "[nltk_data]     C:\\Users\\kashy\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from gensim import models, similarities\n",
        "from gensim.corpora.dictionary import Dictionary\n",
        "# Plus a few other assorted inputs.\n",
        "import numpy as np\n",
        "# We'd typically start by tokenizing the data \n",
        "from gensim.utils import tokenize\n",
        "# And then stem all words.\n",
        "from gensim.parsing.porter import PorterStemmer\n",
        "\n",
        "!pip install pattern\n",
        "import nltk\n",
        "nltk.download('stopwords')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        },
        "id": "Licqozx88OVN",
        "outputId": "7411ceae-3313-4faf-ce40-0eb571843daa"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'It is big and cools the house. It is more powerful than the smaller two-fan models which is why I got it.\\n\\nBUT BE SURE TO CHECK WINDOW SIZE. I could not put it where I intended. Only the largest windows in the living room were big enough.'"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "review_df['reviewText'].iloc[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tl5ZMmI7XEEO",
        "outputId": "264dd293-b446-4644-8c2e-0965a87b05d9"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\kashy\\AppData\\Local\\Temp\\ipykernel_36512\\2745122972.py:3: FutureWarning: The default value of regex will change from True to False in a future version.\n",
            "  review_df['reviewText'] = review_df['reviewText'].str.replace('[^A-Za-z0-9]+', ' ')\n"
          ]
        }
      ],
      "source": [
        "# remove everything from reviewText except alphanumeric characters\n",
        "\n",
        "review_df['reviewText'] = review_df['reviewText'].str.replace('[^A-Za-z0-9]+', ' ')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "vKKpvAODNTC9"
      },
      "outputs": [],
      "source": [
        "# remove all stop words from the reviewText\n",
        "\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "# define a function to remove stop words\n",
        "def remove_stopwords(text):\n",
        "    words = [word.lower() for word in text.split() if word.lower() not in stop_words]\n",
        "    return \" \".join(words)\n",
        "\n",
        "# apply the function to the 'text' column\n",
        "review_df['reviewText'] = review_df['reviewText'].apply(remove_stopwords)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DT-dNG3ioh_Z",
        "outputId": "c220e203-3ae7-49a8-c4a7-de09a1676af5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[['big', 'cools', 'house', 'powerful', 'smaller', 'two', 'fan', 'models', 'got', 'sure', 'check', 'window', 'size', 'could', 'put', 'intended', 'largest', 'windows', 'living', 'room', 'big', 'enough']]\n"
          ]
        }
      ],
      "source": [
        "#create tokenized list of text from each review\n",
        "\n",
        "tokenized_texts = [list(tokenize(text)) for text in review_df['reviewText']]\n",
        "print(tokenized_texts[:1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "473944"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# And then stem all words.\n",
        "from gensim.parsing.porter import PorterStemmer\n",
        "\n",
        "stemmer = PorterStemmer()\n",
        "stemmed_texts = [[stemmer.stem(word) for word in text] for text in tokenized_texts]\n",
        "len(stemmed_texts)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n5M8k14pyEZ8",
        "outputId": "802b53fc-f65f-4e6c-ff4d-c9d355dddfce"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "473746"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# remove the list of words which are empty\n",
        "clean_lemmatized_text = []\n",
        "for doc in stemmed_texts:\n",
        "    # Remove any empty documents\n",
        "    if len(doc)!=0:\n",
        "        clean_lemmatized_text.append(doc)\n",
        "len(clean_lemmatized_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uM_Li0ddkD2d",
        "outputId": "6284659b-d6db-4ae7-ac20-238d6cf89b65"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "473746\n"
          ]
        }
      ],
      "source": [
        "# Create a corpus from a list of texts.\n",
        "from gensim.corpora.dictionary import Dictionary\n",
        "# The dictionary just extracts and numbers each distinct word.\n",
        "dictionary = Dictionary(clean_lemmatized_text, prune_at=20000)\n",
        "# A corpus is a sparse datastore containing the number of times each word appears in each document.\n",
        "corpus = [dictionary.doc2bow(text) for text in clean_lemmatized_text]\n",
        "print(len(corpus))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YnNjN1RdWBzV",
        "outputId": "d8d43618-2cee-4b76-ab85-070922a7b89b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "big\tindex: 0\tcount: 2\n",
            "window\tindex: 19\tcount: 2\n",
            "check\tindex: 1\tcount: 1\n",
            "cool\tindex: 2\tcount: 1\n",
            "could\tindex: 3\tcount: 1\n",
            "enough\tindex: 4\tcount: 1\n",
            "fan\tindex: 5\tcount: 1\n",
            "got\tindex: 6\tcount: 1\n",
            "hous\tindex: 7\tcount: 1\n",
            "intend\tindex: 8\tcount: 1\n"
          ]
        }
      ],
      "source": [
        "# Print a sample of dictionary items.\n",
        "top_words_in_doc_0 = sorted(corpus[0], key=lambda e: e[1], reverse=True)[:10]\n",
        "for word_index, count in top_words_in_doc_0:\n",
        "  print(f'{dictionary[word_index]}\\tindex: {word_index}\\tcount: {count:,}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-V0Th6NkvjSr"
      },
      "source": [
        "# Models trained on test"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "####  Let's start with 10 topics."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "2mOplQRHZyXb"
      },
      "outputs": [],
      "source": [
        "# Build an LDA model.\n",
        "# Note: we could also do this with SKLearn using LinearDiscriminantAnalysis.\n",
        "\n",
        "# Let's start with 10 topics.\n",
        "num_topics = 10\n",
        "model = models.LdaMulticore(corpus, num_topics=num_topics, id2word=dictionary, passes=10, workers=4, dtype=np.float64)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "yB2bTJ9_c78T"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0:  rust pot get like leak clean filter us smell water\n",
            "1:  nice price would cheap expect qualiti pictur color like look\n",
            "2:  small open mug bottl on fit glass cup us lid\n",
            "3:  make work blade like stick get cook candl cut us\n",
            "4:  screw look came togeth put box arriv broken on piec\n",
            "5:  light fan turn us make time clock unit work coffe\n",
            "6:  fit cover soft mattress comfort like wash bed sheet pillow\n",
            "7:  purchas ship on item review amazon order receiv return product\n",
            "8:  back floor would hold get work vacuum us bag chair\n",
            "9:  broke first last year monei time month us work on\n"
          ]
        }
      ],
      "source": [
        "# Let's again look at the most salient words per topic, but note that we no longer\n",
        "# have \"labels\" now.\n",
        "for ix in range(num_topics):\n",
        "  top10 = np.argsort(model.get_topics()[ix])[-10:]\n",
        "  print(f'{ix}:  {\" \".join([dictionary[index] for index in top10])}')  # See any patterns?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "fCbQb7thqn3C"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Coherence metric for number of topics = 10:  0.5571307163553102\n"
          ]
        }
      ],
      "source": [
        "# How coherent are these topics?\n",
        "from gensim.models.coherencemodel import CoherenceModel\n",
        "\n",
        "cm = CoherenceModel(model=model, texts=clean_lemmatized_text, dictionary=dictionary, coherence='c_v')\n",
        "coherence = cm.get_coherence()\n",
        "print(\"Coherence metric for number of topics = 10: \",coherence)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "tyXsZ69Nv11A"
      },
      "outputs": [],
      "source": [
        "# assume you have already trained an LdaMulticore model called `lda_model`\n",
        "# and a Dictionary object called `dictionary`, and a list of reviews called `reviews`\n",
        "num_topics = 10  # number of topics to use for the analysis\n",
        "\n",
        "# create a list to store the reviews for each topic\n",
        "reviews_by_topic = [[] for _ in range(num_topics)]\n",
        "\n",
        "# loop over each review and assign it to its most probable topic\n",
        "for review in clean_lemmatized_text:\n",
        "    bow = dictionary.doc2bow(review)\n",
        "    topic_dist = model.get_document_topics(bow)\n",
        "    topic_id = max(topic_dist, key=lambda x: x[1])[0]\n",
        "    reviews_by_topic[topic_id].append(review)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Topic 0: 25373 reviews\n",
            "\t ['almost', 'explod', 'air', 'vent', 'lock', 'pressur', 'regul', 'never', 'move', 'never', 'releas', 'steam', 'like', 'suppos', 'us', 'metal', 'tong', 'remov', 'pressur', 'regul', 'tremend', 'blast', 'steam', 'escap', 'whew']\n",
            "Topic 1: 87124 reviews\n",
            "\t ['bit', 'small', 'slat', 'bit', 'flimsi']\n",
            "Topic 2: 45190 reviews\n",
            "\t ['ask', 'wan', 'good', 'person', 'man', 'woman', 'god', 'read', 'bibl', 'thing', 'god', 'said', 'god', 'well', 'prayer', 'god', 'well', 'rightou', 'well', 'answer', 'prayer', 'wicket', 'sin', 'god', 'well', 'bless', 'save', 'put', 'god', 'first', 'becus', 'ask', 'faith', 'well', 'happen', 'put', 'trust', 'god', 'count']\n",
            "Topic 3: 34308 reviews\n",
            "\t ['eminem', 'terribl', 'white', 'rapper', 'want', 'peopl', 'laugh', 'feel', 'sorri', 'screw', 'man', 'tri', 'make', 'concept', 'album', 'mix', 'togeth', 'childhood', 'life', 'mix', 'poor', 'parodi', 'celebrati', 'on', 'thing', 'describ', 'littl', 'stori', 'confus', 'hell', 'sh', 'ty', 'skit', 'mention', 'eminem', 'master', 'filler', 'thank', 'skit', 'stupid', 'act', 'sequenc', 'song', 'damn', 'sure', 'poor', 'job', 'artif', 'lenghtin', 'album', 'modern', 'rap', 'gener', 'suck', 'realli', 'want', 'rap', 'album', 'minut', 'long', 'fu', 'good', 'white', 'rapper', 'beasti', 'boi', 'new', 'album', 'suck', 'zack', 'de', 'la', 'rocha', 'rage', 'machin', 'avoid', 'album', 'better', 'life']\n",
            "Topic 4: 61583 reviews\n",
            "\t ['product', 'scratch', 'base', 'also', 'top', 'cover', 'even', 'though', 'bought', 'new', 'item', 'complet', 'satisfi']\n",
            "Topic 5: 37881 reviews\n",
            "\t ['big', 'cool', 'hous', 'power', 'smaller', 'two', 'fan', 'model', 'got', 'sure', 'check', 'window', 'size', 'could', 'put', 'intend', 'largest', 'window', 'live', 'room', 'big', 'enough']\n",
            "Topic 6: 39287 reviews\n",
            "\t ['toooooo', 'small']\n",
            "Topic 7: 34006 reviews\n",
            "\t ['font', 'bigger', 'origin', 'poetri', 'kit', 'look', 'weird', 'pickup', 'line', 'typic', 'fun', 'u', 'open', 'mind', 'luckili', 'rearrang', 'word', 'joke', 'mehhhh', 'sampl', 'joke', 'bui', 'drink', 'want', 'monei', 'heaven', 'must', 'miss', 'angel']\n",
            "Topic 8: 39245 reviews\n",
            "\t ['fit', 'model', 'damag', 'vacuum', 'properli', 'instal', 'belt', 'small', 'much', 'tension', 'vacuum', 'start', 'smoke', 'minut']\n",
            "Topic 9: 69749 reviews\n",
            "\t ['worth', 'bui']\n"
          ]
        }
      ],
      "source": [
        "# print the number of reviews in each topic with top 1 review in each topic\n",
        "for i, reviews in enumerate(reviews_by_topic):\n",
        "    print(f'Topic {i}: {len(reviews)} reviews')\n",
        "    count = 0\n",
        "    for r in reviews:\n",
        "      if count < 1:\n",
        "        print(f'\\t {r}')\n",
        "        count = count+1"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "US_fdouT2Tfp"
      },
      "source": [
        "#### By changing the hyperparameters of LDA model the number of topics to 20 and then 5 to see the impact on coherence.\n",
        "#### Now check for number of topics = 20"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "tCJLnUA6xq-r"
      },
      "outputs": [],
      "source": [
        "# Okay, let's do something more exciting: build an LDA model.\n",
        "# Note: we could also do this with SKLearn using LinearDiscriminantAnalysis.\n",
        "\n",
        "# Let's start with 20 topics.\n",
        "num_topics = 20\n",
        "model1 = models.LdaMulticore(corpus, num_topics=num_topics, id2word=dictionary, passes=10, workers=4, dtype=np.float64)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "yU-RRQOw2_b9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0:  get make strong trai scent us mold ic like smell\n",
            "1:  would star gift first us time order got back on\n",
            "2:  even terribl cheap poor worth product wast bui qualiti monei\n",
            "3:  dai time broke week on last year us month work\n",
            "4:  knife make work get blade hand bowl cut handl us\n",
            "5:  hair carpet suction brush us get rug floor clean vacuum\n",
            "6:  cheapli thin look materi candl feel like cheap pillow made\n",
            "7:  ok smaller much qualiti good would price small size expect\n",
            "8:  air set turn fan time batteri unit clock work light\n",
            "9:  ship came product receiv packag broken box arriv item return\n",
            "10:  shower top seal cup lid mug bottl leak glass water\n",
            "11:  print curtain photo nice white chair like pictur color look\n",
            "12:  fabric soft wash comfort fit cover mattress bag bed sheet\n",
            "13:  tini room poster love purpos kid realli small cute us\n",
            "14:  fall on hold top back assembl screw piec togeth put\n",
            "15:  name lock space close juic video imag contain fit lid\n",
            "16:  link us grind maker make machin filter water cup coffe\n",
            "17:  toaster temperatur microwav oven get us burn hot cook heat\n",
            "18:  star call custom compani read purchas would amazon product review\n",
            "19:  pot dishwash dry rust blanket towel stick pan us wash\n"
          ]
        }
      ],
      "source": [
        "# Let's again look at the most salient words per topic, but note that we no longer\n",
        "# have \"labels\" now.\n",
        "for ix in range(num_topics):\n",
        "  top10 = np.argsort(model1.get_topics()[ix])[-10:]\n",
        "  print(f'{ix}:  {\" \".join([dictionary[index] for index in top10])}')  # See any patterns?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "CgYAxpN53HZm"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Coherence metric for number of topics = 20:  0.5746281338368437\n"
          ]
        }
      ],
      "source": [
        "# How coherent are these topics?\n",
        "from gensim.models.coherencemodel import CoherenceModel\n",
        "\n",
        "cm = CoherenceModel(model=model1, texts=clean_lemmatized_text, dictionary=dictionary, coherence='c_v')\n",
        "coherence = cm.get_coherence()\n",
        "print(\"Coherence metric for number of topics = 20: \",coherence)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "WK0qcoWS3Qhz"
      },
      "outputs": [],
      "source": [
        "# assume you have already trained an LdaMulticore model called `lda_model`\n",
        "# and a Dictionary object called `dictionary`, and a list of reviews called `reviews`\n",
        "num_topics = 20  # number of topics to use for the analysis\n",
        "\n",
        "# create a list to store the reviews for each topic\n",
        "reviews_by_topic = [[] for _ in range(num_topics)]\n",
        "\n",
        "# loop over each review and assign it to its most probable topic\n",
        "for review in clean_lemmatized_text:\n",
        "    bow = dictionary.doc2bow(review)\n",
        "    topic_dist = model1.get_document_topics(bow)\n",
        "    topic_id = max(topic_dist, key=lambda x: x[1])[0]\n",
        "    reviews_by_topic[topic_id].append(review)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Topic 0: 12175 reviews\n",
            "\t ['seem', 'like', 'eminem', 'fallen', 'path', 'rapper', 'rap', 'topic', 'rapper', 'happend', 'rappin', 'bout', 'shroom', 'etc', 'first', 'album', 'quit', 'shame', 'someon', 'differ', 'revolutionairi', 'conform', 'album', 'fade', 'career']\n",
            "Topic 1: 29713 reviews\n",
            "\t ['first', 'tell', 'would', 'take', 'month', 'receiv', 'product', 'right', 'purchas', 'annoi', 'wait', 'product', 'daughter', 'want', 'product', 'final', 'arriv', 'differ', 'item', 'look', 'suppos', 'quilt', 'instead', 'pillow', 'hole', 'could', 'stick', 'hand', 'wai', 'pretti', 'much', 'useless', 'bother', 'return', 'try', 'exchang', 'year', 'old', 'want', 'wait', 'anoth', 'month', 'awar', 'seller']\n",
            "Topic 2: 19124 reviews\n",
            "\t ['worth', 'bui']\n",
            "Topic 3: 36707 reviews\n",
            "\t ['eminem', 'terribl', 'white', 'rapper', 'want', 'peopl', 'laugh', 'feel', 'sorri', 'screw', 'man', 'tri', 'make', 'concept', 'album', 'mix', 'togeth', 'childhood', 'life', 'mix', 'poor', 'parodi', 'celebrati', 'on', 'thing', 'describ', 'littl', 'stori', 'confus', 'hell', 'sh', 'ty', 'skit', 'mention', 'eminem', 'master', 'filler', 'thank', 'skit', 'stupid', 'act', 'sequenc', 'song', 'damn', 'sure', 'poor', 'job', 'artif', 'lenghtin', 'album', 'modern', 'rap', 'gener', 'suck', 'realli', 'want', 'rap', 'album', 'minut', 'long', 'fu', 'good', 'white', 'rapper', 'beasti', 'boi', 'new', 'album', 'suck', 'zack', 'de', 'la', 'rocha', 'rage', 'machin', 'avoid', 'album', 'better', 'life']\n",
            "Topic 4: 26401 reviews\n",
            "\t ['mini', 'processor', 'meant', 'certain', 'small', 'task', 'easili', 'quickli', 'good', 'unit', 'realli', 'like', 'toi', 'light', 'cheapli', 'built', 'feel', 'like', 'on', 'chines', 'thing', 'discount', 'store', 'tv', 'alwai', 'disappoint', 'understand', 'review', 'call', 'workhors', 'cannot', 'simplest', 'task', 'well', 'take', 'chop', 'onion', 'take', 'on', 'small', 'onion', 'cut', 'two', 'section', 'put', 'processor', 'chop', 'without', 'stall', 'blade', 'contain', 'design', 'well', 'enough', 'keep', 'food', 'move', 'consist', 'chop', 'pure', 'ed', 'stick', 'top', 'contain', 'get', 'chop', 'stai', 'reach', 'blade', 'get', 'chop', 'leav', 'larg', 'chunk', 'food', 'intact', 'rest', 'turn', 'pure', 'find', 'chop', 'on', 'onion', 'evenli', 'take', 'lid', 'push', 'food', 'back', 'blade', 'mayb', 'time', 'total', 'pita', 'on', 'step', 'second', 'procedur', 'help', 'add', 'oil', 'liquid', 'cours', 'good', 'knife', 'better', 'faster', 'lesson', 'seem', 'inevit', 'major', 'brand', 'help', 'cover', 'market', 'segment', 'price', 'point', 'even', 'poorli', 'feel', 'immun', 'mistak', 'poor', 'qualiti', 'product', 'hurt', 'long', 'run', 'sure', 'unit', 'cost', 'dollar', 'produc', 'china', 'wholesal', 'make', 'good', 'monei', 'time', 'mark', 'retail', 'almost', 'joke', 'wast', 'monei', 'us', 'qualiti', 'real', 'cuisinart', 'food', 'processor', 'seriou', 'amateur', 'pro', 'disappoint', 'anoth', 'model', 'larger', 'revers', 'blade', 'mai', 'better', 'product', 'know', 'unfortun', 'return', 'gift', 'want', 'make', 'giver', 'feel', 'badli']\n",
            "Topic 5: 14167 reviews\n",
            "\t ['tough', 'clean', 'like', 'stainless', 'cookwar', 'job', 'like', 'better', 'solut', 'found', 'though']\n",
            "Topic 6: 20524 reviews\n",
            "\t ['plastic', 'plastic', 'plastic', 'sai', 'return']\n",
            "Topic 7: 47676 reviews\n",
            "\t ['heater', 'ok', 'get', 'job', 'done', 'small', 'medium', 'size', 'room', 'door', 'close', 'issu', 'heater', 'satisfi']\n",
            "Topic 8: 24957 reviews\n",
            "\t ['big', 'cool', 'hous', 'power', 'smaller', 'two', 'fan', 'model', 'got', 'sure', 'check', 'window', 'size', 'could', 'put', 'intend', 'largest', 'window', 'live', 'room', 'big', 'enough']\n",
            "Topic 9: 28903 reviews\n",
            "\t ['product', 'scratch', 'base', 'also', 'top', 'cover', 'even', 'though', 'bought', 'new', 'item', 'complet', 'satisfi']\n",
            "Topic 10: 22462 reviews\n",
            "\t ['websit', 'box', 'product', 'state', 'capac', 'bowl', 'ounc', 'bare', 'ounc', 'fact', 'put', 'ounc', 'liquid', 'insid', 'measur', 'bowl', 'fill', 'line', 'overflow', 'middl', 'section', 'blade', 'goe', 'liquid', 'spill', 'onto', 'base', 'solid', 'pulver', 'ingredi', 'mai', 'mai', 'spill', 'center', 'overfil', 'sai', 'hold', 'ounc', 'stretch', 'imagin']\n",
            "Topic 11: 31000 reviews\n",
            "\t ['stainless', 'qt', 'year', 'new', 'cooker', 'much', 'lighter', 'love', 'also', 'easier', 'clean', 'lighter']\n",
            "Topic 12: 22426 reviews\n",
            "\t ['pero', 'el', 'resultado', 'final', 'es', 'lo', 'que', 'no', 'tienen', 'acostumbrado', 'asterix', 'su', 'amigo', 'de', 'toda', 'forma', 'considero', 'que', 'vale', 'la', 'pena', 'leerlo', 'por', 'supuesto', 'comprarlo', 'si', 'uno', 'se', 'considera', 'un', 'fan', 'de', 'la', 'seri']\n",
            "Topic 13: 15508 reviews\n",
            "\t ['neat', 'set', 'although', 'us', 'import', 'guest', 'home', 'recommend', 'bachelor']\n",
            "Topic 14: 45240 reviews\n",
            "\t ['sure', 'long', 'flimsi', 'plastic', 'hing', 'last', 'ok', 'keep', 'thing', 'organ']\n",
            "Topic 15: 2392 reviews\n",
            "\t ['lid', 'fit', 'disappoint', 'u', 'sell', 'someth', 'like']\n",
            "Topic 16: 13391 reviews\n",
            "\t ['almost', 'explod', 'air', 'vent', 'lock', 'pressur', 'regul', 'never', 'move', 'never', 'releas', 'steam', 'like', 'suppos', 'us', 'metal', 'tong', 'remov', 'pressur', 'regul', 'tremend', 'blast', 'steam', 'escap', 'whew']\n",
            "Topic 17: 12743 reviews\n",
            "\t ['love', 'hate', 'relationship', 'pot', 'mostli', 'hate', 'bought', 'replac', 'elderli', 'quart', 'mirro', 'ador', 'ruin', 'forget', 'greas', 'lid', 'gasket', 'on', 'dai', 'never', 'ever', 'love', 'extra', 'handl', 'side', 'on', 'fact', 'extra', 'handl', 'combin', 'fond', 'feel', 'old', 'on', 'pick', 'pot', 'big', 'mistak', 'hate', 'shape', 'reason', 'tall', 'narrow', 'instead', 'shape', 'like', 'ordinari', 'pot', 'like', 'old', 'qt', 'mirro', 'matter', 'on', 'fit', 'littl', 'burner', 'electr', 'stove', 'larg', 'front', 'burner', 'on', 'normal', 'want', 'us', 'leav', 'plenti', 'burner', 'uncov', 'accident', 'burn', 'stuff', 'includ', 'joi', 'also', 'weird', 'shape', 'make', 'nigh', 'imposs', 'brown', 'roast', 'pan', 'brown', 'separ', 'skillet', 'stuff', 'thing', 'sidewai', 'yeah', 'chicken', 'stand', 'end', 'also', 'burn', 'sever', 'roast', 'figur', 'pot', 'dumb', 'secret', 'pressur', 'build', 'insid', 'unless', 'red', 'lock', 'button', 'handl', 'pop', 'red', 'button', 'never', 'pop', 'without', 'assist', 'babysit', 'puppi', 'water', 'start', 'bubbl', 'underneath', 'lid', 'run', 'side', 'fizzl', 'burner', 'time', 'start', 'poke', 'prod', 'stupid', 'button', 'us', 'toothpick', 'chore', 'ymmv', 'ye', 'us', 'compress', 'air', 'blow', 'around', 'button', 'wash', 'noth', 'work', 'stick', 'long', 'stori', 'short', 'want', 'burn', 'food', 'blow', 'kitchen', 'stai', 'nearbi', 'pot', 'heat', 'start', 'leak', 'poke', 'poke', 'pry', 'button', 'pop', 'work', 'anyhoo', 'try', 'upload', 'action', 'photo', 'see', 'bare', 'burner', 'stick', 'underneath', 'base', 'streak', 'side', 'water', 'run', 'lid', 'red', 'button', 'stick', 'forc', 'posit', 'honestli', 'could', 'would', 'gotten', 'differ', 'pressur', 'cooker', 'cool', 'extra', 'handl', 'worth', 'frustrat']\n",
            "Topic 18: 29469 reviews\n",
            "\t ['ask', 'wan', 'good', 'person', 'man', 'woman', 'god', 'read', 'bibl', 'thing', 'god', 'said', 'god', 'well', 'prayer', 'god', 'well', 'rightou', 'well', 'answer', 'prayer', 'wicket', 'sin', 'god', 'well', 'bless', 'save', 'put', 'god', 'first', 'becus', 'ask', 'faith', 'well', 'happen', 'put', 'trust', 'god', 'count']\n",
            "Topic 19: 18768 reviews\n",
            "\t ['us', 'insid', 'look', 'like', 'happen', 'know', 'tri', 'scrub', 'realli', 'hard', 'seem', 'perman', 'handl', 'terribl', 'also', 'wobbl']\n"
          ]
        }
      ],
      "source": [
        "# print the number of reviews in each topic\n",
        "for i, reviews in enumerate(reviews_by_topic):\n",
        "    print(f'Topic {i}: {len(reviews)} reviews')\n",
        "    count = 0\n",
        "    for r in reviews:\n",
        "      if count <1:\n",
        "        print(f'\\t {r}')\n",
        "        count = count+1"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "NHG0UPNL2ZWT"
      },
      "source": [
        "#### Now for number of topics = 5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "rqfdwhw22dK2"
      },
      "outputs": [],
      "source": [
        "# Okay, let's do something more exciting: build an LDA model.\n",
        "# Note: we could also do this with SKLearn using LinearDiscriminantAnalysis.\n",
        "\n",
        "# Let's start with 5 topics.\n",
        "num_topics = 5\n",
        "model2 = models.LdaMulticore(corpus, num_topics=num_topics, id2word=dictionary, passes=10, workers=4, dtype=np.float64)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "g0wHSiLc3Jbj"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Coherence metric for number of topics = 5 0.508363460152388\n"
          ]
        }
      ],
      "source": [
        "# How coherent are these topics?\n",
        "from gensim.models.coherencemodel import CoherenceModel\n",
        "\n",
        "cm = CoherenceModel(model=model2, texts=clean_lemmatized_text, dictionary=dictionary, coherence='c_v')\n",
        "coherence = cm.get_coherence()\n",
        "print(\"Coherence metric for number of topics = 5: \",coherence)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "m4t42lXn3XsV"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Topic 0: 56392 reviews\n",
            "\t ['tough', 'clean', 'like', 'stainless', 'cookwar', 'job', 'like', 'better', 'solut', 'found', 'though']\n",
            "Topic 1: 96499 reviews\n",
            "\t ['ask', 'wan', 'good', 'person', 'man', 'woman', 'god', 'read', 'bibl', 'thing', 'god', 'said', 'god', 'well', 'prayer', 'god', 'well', 'rightou', 'well', 'answer', 'prayer', 'wicket', 'sin', 'god', 'well', 'bless', 'save', 'put', 'god', 'first', 'becus', 'ask', 'faith', 'well', 'happen', 'put', 'trust', 'god', 'count']\n",
            "Topic 2: 183100 reviews\n",
            "\t ['product', 'scratch', 'base', 'also', 'top', 'cover', 'even', 'though', 'bought', 'new', 'item', 'complet', 'satisfi']\n",
            "Topic 3: 74556 reviews\n",
            "\t ['almost', 'explod', 'air', 'vent', 'lock', 'pressur', 'regul', 'never', 'move', 'never', 'releas', 'steam', 'like', 'suppos', 'us', 'metal', 'tong', 'remov', 'pressur', 'regul', 'tremend', 'blast', 'steam', 'escap', 'whew']\n",
            "Topic 4: 63199 reviews\n",
            "\t ['big', 'cool', 'hous', 'power', 'smaller', 'two', 'fan', 'model', 'got', 'sure', 'check', 'window', 'size', 'could', 'put', 'intend', 'largest', 'window', 'live', 'room', 'big', 'enough']\n"
          ]
        }
      ],
      "source": [
        "# assume you have already trained an LdaMulticore model called `lda_model`\n",
        "# and a Dictionary object called `dictionary`, and a list of reviews called `reviews`\n",
        "num_topics = 5  # number of topics to use for the analysis\n",
        "\n",
        "# create a list to store the reviews for each topic\n",
        "reviews_by_topic = [[] for _ in range(num_topics)]\n",
        "\n",
        "# loop over each review and assign it to its most probable topic\n",
        "for review in clean_lemmatized_text:\n",
        "    bow = dictionary.doc2bow(review)\n",
        "    topic_dist = model2.get_document_topics(bow)\n",
        "    topic_id = max(topic_dist, key=lambda x: x[1])[0]\n",
        "    reviews_by_topic[topic_id].append(review)\n",
        "\n",
        "# print the number of reviews in each topic\n",
        "for i, reviews in enumerate(reviews_by_topic):\n",
        "    print(f'Topic {i}: {len(reviews)} reviews')\n",
        "    count = 0\n",
        "    for r in reviews:\n",
        "      if count < 1:\n",
        "        print(f'\\t {r}')\n",
        "        count = count+1"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Writing the best model to Pickle file"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### Since LDA model with number of topics = 20 is the model with least coherence, it is the best model and hence we will write this model to pickle file"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pickle\n",
        "filename = 'topic.model'\n",
        "pickle.dump(model1, open(filename, 'wb'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.4"
    },
    "vscode": {
      "interpreter": {
        "hash": "f01cd4171daa3ff16647daf81b09c2074a1bb6e8b1a89a7150d4368b3910c49c"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
